这段话的核心思想是区分 **分词（tokenizing）** 和 **词法分析（Lexing）** 之间的微妙差别，特别是在处理词法单元（tokens）时，是否依赖状态。

我们可以通过以下几个层面来理解：

### 1. **分词（Tokenizing）**

- **分词** 是将输入的字符流（文本）分割成一系列词法单元（tokens）的过程。这个过程通常是无状态的，即每个字符或词段的判断是独立的，不依赖于之前的字符或词段。分词规则是简单的，比如按空格、标点符号等分隔符来切分文本。
- 例如，输入 `int x = 10;`，分词器会把它分割成 `int`、`x`、`=`、`10`、`;` 这些词法单元。

### 2. **词法分析（Lexing）**

- **词法分析** 是一个更加复杂的过程，通常不仅仅是分割文本，还需要判断某些词法单元是否属于特定的类别（比如关键字、标识符、运算符等）。在一些情况下，词法分析会依赖上下文信息，也就是依赖“状态”来做判断。
- 比如，在遇到 `int` 和 `x` 时，词法分析器需要根据上下文判断 `int` 是关键字，`x` 是标识符。词法分析器可能需要保留某些状态（如当前是一个变量声明的上下文），来决定如何解析和识别下一个词法单元。

### 3. **有状态 vs 无状态**

- **无状态的分词** 只依赖当前字符或单词来进行决策，每个决策都是独立的。例如，当你遇到一个空格时，你知道它表示一个新的词法单元，完全不需要关心它前面的内容是什么。
- **有状态的词法分析** 需要依赖上下文信息（即前面已经解析过的内容）来决定当前词法单元的类型。例如，在编程语言中，`if` 可能是一个关键字，也可能是一个标识符，取决于它是否出现在语法规则中作为关键字。因此，词法分析器需要“记住”它之前遇到的内容，才能做出正确的解析。

### 结论：

- **分词** 是一个更简单的过程，通常是无状态的。
- **词法分析** 则更加复杂，通常会涉及到有状态的规则，以便正确解析上下文相关的词法单元。

简言之，词法分析器比分词器更复杂，它不仅仅把字符流切割成词法单元，还会根据上下文来识别这些单元的类型和含义，这就是“有状态”的部分。